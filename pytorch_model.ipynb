{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05f8c4f9-5a7c-4a7f-8f9a-61eb172f3636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef1ceb4a-b52e-4a40-a9e3-f4ee280ce0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "The number of images in a training set is:  50000\n",
      "Files already downloaded and verified\n",
      "The number of images in a test set is:  10000\n",
      "The number of batches per epoch is:  3125\n"
     ]
    }
   ],
   "source": [
    "transformations = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "batch_size = 16\n",
    "number_of_labels = 10 \n",
    "\n",
    "train_set = CIFAR10(root=\"./data\",train=True,transform=transformations,download=True)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "print(\"The number of images in a training set is: \", len(train_loader)*batch_size)\n",
    "\n",
    "test_set = CIFAR10(root=\"./data\", train=False, transform=transformations, download=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "print(\"The number of images in a test set is: \", len(test_loader)*batch_size)\n",
    "print(\"The number of batches per epoch is: \", len(train_loader))\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50c911b3-5605-4f32-8a1b-0a6e9b20e355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model will be running on cpu device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "model = models.resnet18(weights=\"ResNet18_Weights.DEFAULT\")\n",
    "#model = models.alexnet(weights=\"AlexNet_Weights.DEFAULT\")\n",
    "#model = models.squeezenet1_0(weights=\"SqueezeNet1_0_Weights.DEFAULT\")\n",
    "#model = models.vgg16(weights=\"VGG16_Weights.DEFAULT\")\n",
    "\n",
    "\n",
    "#model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#model = torch.hub.load(\"pytorch/vision\", \"resnet50\", weights=\"IMAGENET1K_V2\")\n",
    "#model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', weights=None)\n",
    "#model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', weights=None)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"The model will be running on\", device, \"device\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "327de99b-266d-456c-9401-58f77415ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "# Define the loss function and an optimizer with Adam optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.01, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50364760-f5ab-4335-9e04-9cfdcd09e68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 2.343\n",
      "[1,  2000] loss: 2.118\n",
      "[1,  3000] loss: 2.075\n",
      "54.58889722824097  seconds for  10000.0  samples of testing data\n",
      "For epoch  1  the test accuracy over the whole test set is 14 %\n",
      "For epoch  1  the training time is :  822.8622088432312\n",
      "[2,  1000] loss: 20321373716692926493360128.000\n",
      "[2,  2000] loss: 216410565563257065796796416.000\n",
      "[2,  3000] loss: 119410222722304048187310080.000\n",
      "69.05940985679626  seconds for  10000.0  samples of testing data\n",
      "For epoch  2  the test accuracy over the whole test set is 9 %\n",
      "For epoch  2  the training time is :  1021.1753861904144\n",
      "[3,  1000] loss: 22484880928864244810947493888.000\n",
      "[3,  2000] loss: 5284426074046259744173195264.000\n",
      "[3,  3000] loss: 21892164275881246769459232768.000\n",
      "101.15141296386719  seconds for  10000.0  samples of testing data\n",
      "For epoch  3  the test accuracy over the whole test set is 12 %\n",
      "For epoch  3  the training time is :  1227.0694682598114\n",
      "[4,  1000] loss: 24555591731906389360806723584.000\n",
      "[4,  2000] loss: 38044277087990681924410540032.000\n",
      "[4,  3000] loss: 28800061058517638965545140224.000\n",
      "70.87767887115479  seconds for  10000.0  samples of testing data\n",
      "For epoch  4  the test accuracy over the whole test set is 11 %\n",
      "For epoch  4  the training time is :  1442.976567029953\n",
      "[5,  1000] loss: 30229156760076673752689016832.000\n",
      "[5,  2000] loss: 87242115732971960673945780224.000\n",
      "[5,  3000] loss: 128932573796580319961158778880.000\n",
      "61.54079222679138  seconds for  10000.0  samples of testing data\n",
      "For epoch  5  the test accuracy over the whole test set is 10 %\n",
      "For epoch  5  the training time is :  1194.7872109413147\n",
      "[6,  1000] loss: 127522800186390982346076585984.000\n",
      "[6,  2000] loss: 84177046312218291771008876544.000\n",
      "[6,  3000] loss: 66071473619075175149545193472.000\n",
      "62.187742948532104  seconds for  10000.0  samples of testing data\n",
      "For epoch  6  the test accuracy over the whole test set is 12 %\n",
      "For epoch  6  the training time is :  912.4066870212555\n",
      "[7,  1000] loss: 149417160250087844781880246272.000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Let's build our model\n",
    "    train(model=model, num_epochs=20, train_loader=train_loader, test_loader=test_loader, optimizer=optimizer, loss_fn=loss_fn)\n",
    "    print('Finished Training')\n",
    "\n",
    "    # Test which classes performed well\n",
    "    testClassess()\n",
    "    \n",
    "    # Let's load the model we just created and test the accuracy per label\n",
    "    model = model\n",
    "    path = \"myFirstModel.pth\"\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "    # Test with batch of images\n",
    "    testBatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b47e166-1db6-4783-976c-a73ae28cb2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "testAccuracy(model, test_loader)\n",
    "end_time = time.time()\n",
    "print(end_time-start_time, \" seconds for testing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fb10e1-e20b-499e-b73a-9096bfd14e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d26e482-9e93-4da9-b7b6-6bc94bf5da38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
